# ğŸ”Œ pyllmlib

[![PyPI Downloads](https://static.pepy.tech/badge/pyllmlib)](https://pepy.tech/projects/pyllmlib)
[![PyPI Version](https://img.shields.io/pypi/v/pyllmlib.svg)](https://pypi.org/project/pyllmlib/)
![Python Version](https://img.shields.io/pypi/pyversions/pyllmlib.svg)
[![License](https://img.shields.io/github/license/yazirofi/pyllmlib)](LICENSE)
[![GitHub Stars](https://img.shields.io/github/stars/yazirofi/pyllmlib?style=social)](https://github.com/yazirofi/pyllmlib)

---

<div style="
  width:200px; 
  height:100px; 
  overflow:hidden; 
  display:flex; 
  justify-content:center; 
  align-items:center; 
  margin:auto; 
  position:relative; 
  top:0; 
  bottom:0; 
  left:0; 
  right:0; 
  border-radius:10px; 
  box-shadow:0 4px 8px rgba(0,0,0,0.2);
">
  <img src="https://raw.githubusercontent.com/yazirofi/cdn/main/yazirofi.jpg" 
       style="width:100%; height:100%; object-fit:cover;" 
       alt="Cropped Image">
</div>

---

**pyllmlib** is a lightweight, provider-agnostic Python package that gives you **one simple interface** to work with multiple Large Language Model (LLM) APIs.

Stop juggling different SDKs and client libraries â€” whether itâ€™s **OpenAI GPT**, **Google Gemini**, **Mistral AI**, or **Groq**, you write your code once and switch providers in seconds.

ğŸ¯ **Ideal for** developers who want to:

* Experiment with different LLMs quickly
* Build multi-provider AI applications
* Avoid vendor lock-in with a consistent API

---

## âœ¨ Features

* ğŸ”Œ **Unified API** â€” A single `generate()` function for all providers
* ğŸŒ **Multi-Provider Support** â€” OpenAI, Gemini, Mistral, Groq (with more coming soon)
* ğŸ§  **Consistent Message Format** â€” Same request style across providers
* ğŸ” **Flexible Config** â€” Use env vars, inline setup, or config files
* ğŸ“¦ **Minimal Dependencies** â€” Only needs `requests`
* ğŸ”„ **Quick Provider Switching** â€” Change models with one line
* ğŸ›¡ï¸ **Automatic Token Handling** â€” Prevents overflows & context errors
* ğŸ“œ **Role-Based Conversations** â€” System, user, assistant messages
* ğŸ”§ **Extensible** â€” Add your own providers with minimal code
* ğŸš€ **No Vendor Lock-In** â€” Swap providers without rewriting logic

---

## ğŸ“¦ Installation

From PyPI (recommended):

```bash
pip install pyllmlib
```

From GitHub:

```bash
# Latest release
pip install git+https://github.com/yazirofi/pyllmlib.git

# Development version
git clone https://github.com/yazirofi/pyllmlib.git
cd pyllmlib
pip install -e .
```

**Requirements**:

* Python 3.7+
* `requests` (installed automatically)

---

## ğŸš€ Quick Start

```python
from pyllmlib import config, generate

# Configure your preferred LLM
config(
    provider="openai",
    api_key="your-openai-api-key",
    model="gpt-4"
)

# Generate text
response = generate("Explain quantum computing in simple terms")
print(response)
```

âœ… Same code works with **any provider** â€” just change the config.

---

## âš™ï¸ Configuration

### 1. Direct in Code

```python
from pyllmlib import config

# OpenAI
config(provider="openai", api_key="sk-...", model="gpt-4")

# Google Gemini
config(provider="gemini", api_key="AIza...", model="gemini-2.5-flash")

# Mistral
config(provider="mistral", api_key="...", model="mistral-large-latest")

# Groq
config(provider="groq", api_key="gsk_...", model="mixtral-8x7b-32768")
```

### 2. Environment Variables

```bash
LLM_PROVIDER=openai
LLM_API_KEY=sk-your-openai-key
LLM_MODEL=gpt-4
LLM_BASE_URL=https://api.openai.com/v1  # Optional
```

```python
from pyllmlib import config, generate

config()  # Loads from env
print(generate("What is LLM?"))
```

---

## ğŸ’¬ Usage Examples

### Text Generation

```python
from pyllmlib import config, generate

config(provider="gemini", api_key="AIza...", model="gemini-2.5-flash")

print(generate("What is the capital of France?"))

prompt = """
Write a Python function to calculate factorial with error handling and docstring.
"""
print(generate(prompt))
```

### Interactive Chat

```python
from pyllmlib import config, chat, reset_chat

config(provider="gemini", api_key="AIza...", model="gemini-2.5-flash")

while (q := input("Ask: ")):
    print(chat(q))

reset_chat()
```

---

## ğŸŒ Supported Providers

### âœ… Currently Supported

* **OpenAI** â€” `gpt-4`, `gpt-4-turbo`, `gpt-3.5-turbo`
* **Google Gemini** â€” `gemini-2.5-flash`, `gemini-1.5-flash`
* **Mistral AI** â€” `mistral-large-latest`, `mistral-small-latest`
* **Groq** â€” `mixtral-8x7b-32768`, `llama2-70b-4096`, `gemma-7b-it`

### ğŸ”œ Coming Soon

* Anthropic Claude
* Cohere
* Ollama & LM Studio (local hosting)
* Hugging Face models

---

## ğŸ› Troubleshooting

* **Auth Errors** â†’ Check API key format
* **Model Not Found** â†’ Verify model name is correct

---

## ğŸ“Š Best Practices

```python
# âœ… Reuse config for multiple prompts
config(provider="openai", api_key="sk-...", model="gpt-4")
for p in prompts:
    print(generate(p))

# âŒ Donâ€™t reconfigure on every request
```

ğŸ’¡ **Cost Optimization**: Use `gpt-3.5-turbo` for simple tasks, `gpt-4` for complex ones.

---

## ğŸ“š API Reference

* `config(**kwargs)` â†’ Set provider, API key, model
* `generate(prompt, **kwargs)` â†’ Single text output
* `generate_stream(prompt, **kwargs)` â†’ Streaming output
* `chat(message)` â†’ Conversational interface
* `reset_chat()` â†’ Clear conversation history

---


## ğŸ“„ License

Licensed under the **MIT License** â€“ see [LICENSE](LICENSE).

---

## ğŸ‘¨ğŸ’» Author

**Shay Yazirofi**

* GitHub: [@yazirofi](https://github.com/yazirofi)
* Email: [yazirofi@gmail.com](mailto:yazirofi@gmail.com)

---

â­ If you find **pyllmlib** useful, please **star the repo** on GitHub!
ğŸ“– More docs & tutorials: [Wiki](https://github.com/yazirofi/pyllmlib/)

---

